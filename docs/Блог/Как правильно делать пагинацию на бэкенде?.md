---
date: 2023-06-18
share: true
hide:
  - navigation
  - toc
---

Пагинацией называется разделение большого массива данных на отдельные страницы для удобства использования. Это может выглядеть как кнопки "Предыдущая" и "Следующая" с рядом номеров страниц в результатах поиска на google.com, так и как непрерывная лента с постами в instagram.com. В обоих случаях пагинация нам нужна, потому что мы не можем загрузить в браузер все результаты поиска целиком.

`Spring Data Jpa` для реализации пагинации предлагает использовать интерфейс `PagingAndSortingRepository`:

```java
public interface ProductRepository extends PagingAndSortingRepository<Product, Integer> {
    List<Product> findAll(Pageable pageable);
}
```

В `Pageable` мы передаем нужный нам номер страницы, ее размер и поле для сортировки, без которого пагинация не имеет смысла.
Но, стандартное решение от `Spring` подходит не всегда. Вызов метода `findAll` приведет приблизительно к следующему SQL запросу:

```sql
SELECT * FROM product ORDER BY id LIMIT 10 OFFSET 20
```

Значения `LIMIT` и `OFFSET` будут выбраны в зависимости от того, какую по счету страницу запрашиваем. Этот подход называется **offset pagination**.
Проблема в том, что при использовании конструкции `OFFSET N` БД все равно выбирает все начальные строки запроса, и только после выборки пропускает `N` первых строк. Из за этой особенности после страницы 20-30 запрос начинает работать все хуже и хуже. Чтобы этого избежать стоит использовать подход **keyset/seek/cursor pagination)**. Суть подхода в том, что мы по некоторому полю (например по `id`) убираем из результата уже показанные пользователю записи:

```sql
SELECT id FROM product WHERE id > 478 LIMIT 10
```

Таким образом мы получаем одинаковую скорость выборки не зависимо от номера страницы. Из минусов - теряем возможность сразу перейти на произвольную страницу.

## Как работать с большими ResultSet без пагинации?

Допустим  вам нужно сделать выгрузку таблицы на миллионы записей в файл на диск, но по какой-то причине пагинация не подходит.
По умолчанию многие JDBC драйверы (PostgreSQL, MySQL) загружают в память приложения весь результат запроса из БД, что в нашем случае может привести к падению приложения из за нехватки памяти. Чтобы этого избежать мы можем настроить JDBC драйвер таким образом, чтобы он загружал данные небольшими порциям, пока мы последовательно проходим по ним курсором.
Как это сделать - лучше прочитать в документации на ваш JDBC драйвер, 100% универсального решения нет. 

В большинстве случаев достаточно установить параметр запроса `FetchSize`, например, используя `JdbcTemplate` из `Spring`:
```java
JdbcTemplate jdbcTemplate = new JdbcTemplate(dataSource);
jdbcTemplate.setFetchSize(200);
jdbcTemplate.query("select * from product", rs -> addRowToExcel(rs));
```

При таких настройках JDBC драйвер будет запрашивать результаты запроса пакетами по 200 строк, причем следующий пакет будет загружен только когда мы обработаем все строки из текущего.

>[!info]
>В Oracle JDBC драйвере по умолчанию для всех запросов установлен `fetch size=10`. То есть, если вы запускаете `select`, который возвращает 50 строк, то потребуется 5 сетевых запросов от клиента в БД , чтобы выбрать все данные. Попробуйте установить больший fetch size, и, скорее всего, это положительно скажется на производительности.

## Как работать с большими ResultSet без пагинации в Spring Data JPA?

JPA для стриминга больших ResultSet из БД подходит плохо, слишком много способов отстрелить себе обе ноги, плюс плохая производительность. Но, если вы не ищите легких путей, репозиторий будет выглядеть так:

```java
public interface ProductRepository extends Repository<Product, Long> {
    
    @QueryHints(value = {
            @QueryHint(name = HINT_FETCH_SIZE, value = "50"),
            @QueryHint(name = HINT_CACHEABLE, value = "false"),
            @QueryHint(name = READ_ONLY, value = "true")
    })
    @Query("select p from Product p")
    Stream<Product> getAll();
}
```

Аннотациями `@QueryHint` меняем стандартное поведение `Hibernate`:
- **HINT_FETCH_SIZE** - просим загружать из БД результат запроса пачками по 50 записей
- **HINT_CACHEABLE** - на всякий случай отключаем кеширование `Product` в кеше второго уровня Hibernate.
- **READ_ONLY** - убеждаем, что полученные `Product` будем использовать только для чтения. Это избавляет `Hibernate` от необходимости следить за изменениями состояния `Product`, что положительно скажется на производительности.

Но пачкой аннотаций история не заканчивается, важно еще правильно вызывать этот метод:

```java
  @Transactional(readOnly = true)
  public void processProducts() {
    try(Stream<Book> stream = productRepository.getAll()) {
      stream.forEach(product -> {
        addRowToExcel(product);
        entityManager.detach(product);
      });
    }
  }
```

Обратите внимание на несколько моментов. 
1. Все действия со `Stream` нужно делать в транзакции, иначе сессия с БД закроется и никаких данных мы не получим.
2. `Stream` необходимо закрывать самостоятельно, об этом нас просит документация `Spring Data Jpa`. Хотя на практике можно этого и не делать, `Hibernate` закроет его за нас в конце сессии.
3. `Hibernate` держит в кэше первого уровня все полученные в транзакции объекты. Поэтому вручную выкидываем каждую `entity` из контекста, как только закончили ее обработку. Для этого вызываем `entityManager.detach(product)`. Иначе на больших выборках приложение будет падать с `OutOfMemoryException`. Альтернативный и более производительный вариант - каждые N итераций вызывать полную очистку контекста функцией `EntityManager.clean()`. 
   
## Ссылки
- [OFFSET is bad for skipping previous rows](https://use-the-index-luke.com/sql/partial-results/fetch-next-page)